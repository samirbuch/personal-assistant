import Twilio from "twilio";
import * as Bun from "bun";
import { createClient, LiveTranscriptionEvents, LiveTTSEvents, LiveClient, SpeakLiveClient } from "@deepgram/sdk";
import cleanPublicURL from "./util/cleanPublicURL";
import { TwilioWebsocket } from "../lib/TwilioWebsocketTypes";

import { Experimental_Agent as Agent, stepCountIs, tool, type ModelMessage } from "ai";
import { anthropic } from "@ai-sdk/anthropic";
import ResettableAbortController from "./util/ResettableAbortController";

const PORT = process.env.PORT || 40451;

console.log(`[${new Date().toISOString()}] Hello via Bun!`);

interface CallSession {
  ws: Bun.ServerWebSocket;
  deepgramSTT: LiveClient;
  deepgramTTS: SpeakLiveClient;
  conversation: ModelMessage[];
  agent: Agent<{}, never, never>;
  abortController: ResettableAbortController;
  isStreaming: boolean;
  currentAssistantMessage: string; // Track the message being generated
  isProcessingInterrupt: boolean; // Prevent re-entrant interrupts
  noiseBaseline: number; // Rolling average of background noise
  noiseSamples: number[]; // Recent noise samples for averaging
}

const callSessions = new Map<string, CallSession>();

const client = Twilio(process.env.TWILIO_SID, process.env.TWILIO_AUTH_TOKEN);
const deepgramClient = createClient(process.env.DEEPGRAM_ACCESS_TOKEN);

const tools = {

}

// Calculate audio energy from mulaw audio
function calculateAudioEnergy(mulawBase64: string): number {
  try {
    const buffer = Uint8Array.fromBase64(mulawBase64);
    let energy = 0;
    for (let i = 0; i < buffer.length; i++) {
      const sample = buffer[i];
      if (sample === undefined) continue;
      const deviation = Math.abs(sample - 127);
      energy += deviation;
    }
    return energy / buffer.length;
  } catch {
    return 0;
  }
}

// Update rolling average of noise baseline
function updateNoiseBaseline(session: CallSession, energy: number) {
  session.noiseSamples.push(energy);
  
  // Keep only the last 50 samples (about 1 second of audio at 20ms chunks)
  if (session.noiseSamples.length > 50) {
    session.noiseSamples.shift();
  }
  
  // Calculate rolling average
  const sum = session.noiseSamples.reduce((a, b) => a + b, 0);
  session.noiseBaseline = sum / session.noiseSamples.length;
}

// Check if audio energy indicates speech (above baseline + margin)
function isSpeechDetected(session: CallSession, energy: number): boolean {
  // Need a margin above baseline to detect speech vs noise
  // Factor of 2-3x baseline is typically good for speech detection
  const threshold = session.noiseBaseline * 2.5;
  return energy > Math.max(threshold, 15); // Minimum absolute threshold of 15
}

// Helper to detect if mulaw audio contains potential speech (simple energy-based detection)
function hasAudioEnergy(mulawBase64: string): boolean {
  try {
    const buffer = Uint8Array.fromBase64(mulawBase64);
    let energy = 0;
    for (let i = 0; i < buffer.length; i++) {
      // Mulaw decode approximation - just check if values deviate from silence (127/255)
      const sample = buffer[i];
      if (sample === undefined) continue;
      const deviation = Math.abs(sample - 127);
      energy += deviation;
    }
    const avgEnergy = energy / buffer.length;
    // Threshold for detecting speech vs silence (tune this as needed)
    return avgEnergy > 10; // Adjust threshold based on testing
  } catch {
    return false;
  }
}

function handleInterrupt(streamSid: string, utteranceUntilInterrupt: string) {
  const session = callSessions.get(streamSid);
  if (!session) {
    console.error(`No session found for streamSid: ${streamSid}`);
    return;
  }

  const conversation = session.conversation;
  let updatedConversation = [...conversation];
  const interruptedIndex = updatedConversation.findLastIndex(
    (message) =>
      message.role === "assistant" &&
      message.content &&
      typeof message.content === "string" &&
      message.content.includes(utteranceUntilInterrupt)
  );

  if (interruptedIndex !== -1) {
    const interruptedMessage = updatedConversation[interruptedIndex];
    if (!interruptedMessage) return;

    const content = typeof interruptedMessage.content === "string"
      ? interruptedMessage.content
      : "";

    const interruptPosition = content.indexOf(utteranceUntilInterrupt);
    const truncatedContent = content.substring(
      0,
      interruptPosition + utteranceUntilInterrupt.length
    );

    updatedConversation[interruptedIndex] = {
      ...interruptedMessage,
      content: truncatedContent,
    } as ModelMessage;

    updatedConversation = updatedConversation.filter(
      (message, index) =>
        !(index > interruptedIndex && message.role === "assistant")
    );
  }

  session.conversation = updatedConversation;
  callSessions.set(streamSid, session);
}

Bun.serve({
  port: PORT,
  routes: {
    "/api/calls/:number": async req => {
      if (req.method !== "POST") {
        return new Response("Error 405: Method not allowed", { status: 405 });
      }

      const from = process.env.TWILIO_PHONE_NUMBER;
      if (!from) return new Response("Error 500: Missing TWILIO_PHONE_NUMBER", { status: 500 });

      const publicURL = cleanPublicURL(process.env.PUBLIC_URL);
      if (!publicURL) return new Response("Error 500: Missing or malformed PUBLIC_URL", { status: 500 });

      const isNgrok = publicURL.includes(".ngrok-free.app");
      const url = `http://${publicURL}${isNgrok ? "" : `:${PORT}`}/api/twilio-gateway`;

      console.log(`Calling ${from}`);
      const call = await client.calls.create({
        from,
        to: req.params.number,
        url: url
      });

      console.log("Call!", call);

      return new Response(JSON.stringify(call.toJSON()));
    },
    "/api/twilio-gateway": async req => {
      const publicURL = cleanPublicURL(process.env.PUBLIC_URL);
      if (!publicURL) return new Response("Error 500: Missing or malformed PUBLIC_URL", { status: 500 });

      const isNgrok = publicURL.includes(".ngrok-free.app");
      const ws = `wss://${publicURL}${isNgrok ? "" : `:${PORT}`}/twilio-ws`;

      const response = new Twilio.twiml.VoiceResponse();
      const connect = response.connect();
      connect.stream({
        name: "Inbound Audio Stream",
        url: ws
      });
      console.log("Forwarding twilio call stream to:", ws);

      return new Response(
        response.toString(),
        {
          headers: {
            "Content-Type": "text/xml"
          }
        }
      );
    }
  },

  fetch(req, server) {
    const url = new URL(req.url);
    console.log(url);
    if (url.pathname === "/twilio-ws") {
      const upgraded = server.upgrade(req);
      if (!upgraded) {
        return new Response("Upgrade failed", { status: 400 });
      }
    }
    // return new Response("Hello World");
  },
  websocket: {
    open(ws) {
      console.log("Websocket connection opened!");
    },
    message(ws, message) {
      const json: TwilioWebsocket.Message = JSON.parse(message as string);

      switch (json.event) {
        case "start": {
          console.log("START EVENT:", json);

          // Create dedicated Deepgram connections for this call
          const deepgramSTT = deepgramClient.listen.live({
            model: "nova-3",
            encoding: "mulaw",
            sample_rate: 8000,
            interim_results: true
          });

          const deepgramTTS = deepgramClient.speak.live({
            model: "aura-2-thalia-en",
            encoding: "mulaw",
            sample_rate: 8000
          });

          const abortController = new ResettableAbortController();

          const agent = new Agent({
            model: anthropic("claude-3-5-haiku-latest"),
            system: `
            You are a helpful personal assistant named Jordan. You work for your client, Samir Buch. A server is sending your responses over a phone call to wherever it is Samir needs an appointment. Chances are, the person on the other end of the call will want to help Samir. It should be made clear that it's not necessarily your goal to help the person on the phone call, more so it's your goal to help Samir book an appointment with them.

            Samir uses he/him or they/them pronouns.
            
            Do not give out the following informaton unless requested: his phone number is 267-625-3752, and his preferred email is samirjbuch@gmail.com.
            Please note that your responses will be converted into audio, so take extra care to: spell out numbers, and avoid using punctuation that doesn't translate well into speech (including but not limited to slashes, brackets, braces, parentheses, or bullet points).
            Please keep all responses very brief.
            `,
            tools,
            stopWhen: stepCountIs(20),
            abortSignal: abortController.signal
          });

          // Create session for this call
          const session: CallSession = {
            ws,
            deepgramSTT,
            deepgramTTS,
            conversation: [],
            agent,
            abortController,
            isStreaming: false,
            currentAssistantMessage: "",
            isProcessingInterrupt: false,
            noiseBaseline: 10, // Initial baseline
            noiseSamples: []
          };

          callSessions.set(json.streamSid, session);

          // Set up STT event handlers for this specific call
          deepgramSTT.on(LiveTranscriptionEvents.Open, () => {
            console.log(`[${json.streamSid}] Deepgram STT ready`);
          });

          // Text received from the user. Send this to the LLM.
          deepgramSTT.on(LiveTranscriptionEvents.Transcript, async (data) => {
            const transcript = data.channel?.alternatives?.[0]?.transcript;

            // Handle final transcripts
            if (data.is_final && data.speech_final) {
              console.log(`[${json.streamSid}] Transcript: ${transcript}`)
              if (transcript && !session.isStreaming) { // Only process if not already streaming
                session.conversation.push({
                  role: "user",
                  content: transcript
                });

                console.log(`[${json.streamSid}] Conversation:`, session.conversation);

                session.isStreaming = true;
                session.isProcessingInterrupt = false; // Clear interrupt flag
                session.currentAssistantMessage = "";
                
                // Stream text back from LLM
                try {
                  const result = agent.stream({
                    prompt: session.conversation
                  });

                  let fullText = ""
                  // Stream the streamed text to deepgram TTS
                  for await (const chunk of result.textStream) {
                    // Check if we were interrupted during streaming
                    if (!session.isStreaming) {
                      console.log(`[${json.streamSid}] Stream cancelled mid-generation`);
                      break;
                    }
                    console.log(chunk);
                    session.deepgramTTS.sendText(chunk);
                    fullText += chunk;
                    session.currentAssistantMessage = fullText; // Track progress
                  }
                  
                  // Only flush and save if we completed without interruption
                  if (session.isStreaming) {
                    session.deepgramTTS.flush();
                    session.isStreaming = false;
                    session.conversation.push({
                      role: "assistant",
                      content: fullText
                    });
                    session.currentAssistantMessage = "";
                  }
                } catch (error: any) {
                  if (error.name === 'AbortError' || error.message?.includes('interrupted')) {
                    console.log(`[${json.streamSid}] LLM stream aborted (interrupted)`);
                    // Don't add to conversation - it was interrupted
                  } else {
                    console.error(`[${json.streamSid}] Error in LLM stream:`, error);
                    session.isStreaming = false;
                    session.currentAssistantMessage = "";
                  }
                }
              } else if (session.isStreaming) {
                console.log(`[${json.streamSid}] Ignoring transcript - already streaming`);
              }
            }
          });

          deepgramSTT.on(LiveTranscriptionEvents.Error, (error) => {
            console.error(`[${json.streamSid}] STT Error:`, error);
          });

          deepgramSTT.on(LiveTranscriptionEvents.Close, () => {
            console.log(`[${json.streamSid}] STT connection closed`);
            session.abortController.abort();
          });

          // Set up TTS event handlers for this specific call
          deepgramTTS.on(LiveTTSEvents.Open, () => {
            console.log(`[${json.streamSid}] Deepgram TTS ready`);
          });

          // Audio converted from LLM. Send this to Twilio.
          deepgramTTS.on(LiveTTSEvents.Audio, (audio: Uint8Array) => {
            // console.log(`[${json.streamSid}] Received TTS audio:`, audio.byteLength, "bytes");

            // Convert the mulaw audio to base64
            const b64 = audio.toBase64();
            const msg: TwilioWebsocket.Sendable.MediaMessage = {
              event: "media",
              streamSid: json.streamSid,
              media: { payload: b64 }
            };
            session.ws.send(JSON.stringify(msg));
          });

          deepgramTTS.on(LiveTTSEvents.Error, (error) => {
            console.error(`[${json.streamSid}] TTS Error:`, error);
          });

          deepgramTTS.on(LiveTTSEvents.Close, () => {
            console.log(`[${json.streamSid}] TTS connection closed`);
          });

          break;
        }
        // Audio received from the user. Send this to deepgram.
        case "media": {
          const session = callSessions.get(json.streamSid);
          if (!session) {
            console.error(`No session found for streamSid: ${json.streamSid}`);
            break;
          }

          // Calculate energy for this audio chunk
          const energy = calculateAudioEnergy(json.media.payload);
          
          // Update noise baseline when NOT streaming (learn ambient noise)
          if (!session.isStreaming) {
            updateNoiseBaseline(session, energy);
          }

          // IMMEDIATE interruption detection on incoming audio
          if (session.isStreaming && !session.isProcessingInterrupt && isSpeechDetected(session, energy)) {
            console.log(`[${json.streamSid}] INTERRUPTION DETECTED (energy: ${energy.toFixed(2)}, baseline: ${session.noiseBaseline.toFixed(2)})`);
            
            // Set flag to prevent re-entrant interrupts
            session.isProcessingInterrupt = true;
            
            // 1. Abort the current LLM stream
            session.abortController.abort("User interrupted");
            
            // 2. STOP TTS immediately - close connection
            session.deepgramTTS.requestClose();
            
            // 3. Send Twilio clear command to stop playing buffered audio
            const clearMsg: TwilioWebsocket.Sendable.ClearMessage = {
              event: "clear",
              streamSid: json.streamSid
            };
            session.ws.send(JSON.stringify(clearMsg));
            
            // 4. Wait a tiny bit for cleanup, then create new TTS connection
            setTimeout(() => {
              const newTTS = deepgramClient.speak.live({
                model: "aura-2-thalia-en",
                encoding: "mulaw",
                sample_rate: 8000
              });
              
              // Set up TTS event handlers for new connection
              newTTS.on(LiveTTSEvents.Open, () => {
                console.log(`[${json.streamSid}] Deepgram TTS ready (after interrupt)`);
              });

              newTTS.on(LiveTTSEvents.Audio, (audio: Uint8Array) => {
                // Only send audio if not currently being interrupted
                if (!session.isProcessingInterrupt) {
                  const b64 = audio.toBase64();
                  const msg: TwilioWebsocket.Sendable.MediaMessage = {
                    event: "media",
                    streamSid: json.streamSid,
                    media: { payload: b64 }
                  };
                  session.ws.send(JSON.stringify(msg));
                }
              });

              newTTS.on(LiveTTSEvents.Error, (error) => {
                console.error(`[${json.streamSid}] TTS Error:`, error);
              });

              newTTS.on(LiveTTSEvents.Close, () => {
                console.log(`[${json.streamSid}] TTS connection closed`);
              });
              
              session.deepgramTTS = newTTS;
            }, 50); // Small delay to ensure cleanup
            
            // 5. Update conversation with partial assistant response
            if (session.currentAssistantMessage) {
              handleInterrupt(json.streamSid, session.currentAssistantMessage);
            }
            
            // 6. Reset abort controller for next response
            session.abortController.reset();
            
            // 7. Mark streaming as complete
            session.isStreaming = false;
            session.currentAssistantMessage = "";
            
            // 8. Clear interrupt flag after a short delay
            setTimeout(() => {
              session.isProcessingInterrupt = false;
            }, 200); // Prevent rapid re-triggers
          }

          // Send audio to Deepgram STT for this specific call
          const base64ToUint8Array = Uint8Array.fromBase64(json.media.payload);
          session.deepgramSTT.send(base64ToUint8Array.buffer);

          break;
        }

        case "stop": {
          console.log(`[${json.streamSid}] User hung up!`);
          const session = callSessions.get(json.streamSid);

          if (session) {
            // Clean up Deepgram connections
            session.deepgramSTT.requestClose();
            session.deepgramTTS.requestClose();
            callSessions.delete(json.streamSid);
            session.abortController.abort();
          }

          break;
        }
      }
    }
  }
})